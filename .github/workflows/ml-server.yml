name: ml-server

on:
  push:
    branches: ["main"]
    paths:
      - "ml-server/**"
      - ".github/workflows/ml-server.yml"

jobs:
  build-and-push-ml:
    runs-on: ubuntu-24.04-arm

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3 # 최신

      - name: Login to DockerHub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build and Push ML Image (ARM native)
        uses: docker/build-push-action@v6 # 최신
        with:
          context: ./ml-server
          file: ./ml-server/Dockerfile
          platforms: linux/arm64
          push: true
          tags: |
            ${{ secrets.DOCKERHUB_USERNAME }}/ml-server:latest
            ${{ secrets.DOCKERHUB_USERNAME }}/ml-server:arm64
          cache-from: type=gha # GHA 캐시로 전환
          cache-to: type=gha,mode=max
          provenance: false #  이미지 메타데이터 생략해 약간 단축

  deploy:
    runs-on: ubuntu-latest
    needs: build-and-push-ml

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Send files to server
        uses: appleboy/scp-action@master
        with:
          host: ${{ secrets.SSH_HOST }}
          username: ${{ secrets.SSH_USERNAME }}
          key: ${{ secrets.SSH_KEY }}
          port: ${{ secrets.SSH_PORT }}
          source: "docker-compose.yml, nginx.conf"
          target: /home/ubuntu/interview
          overwrite: true
          debug: true

      - name: Deploy to server
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.SSH_HOST }}
          username: ${{ secrets.SSH_USERNAME }}
          key: ${{ secrets.SSH_KEY }}
          port: ${{ secrets.SSH_PORT }}
          script: |
            cd /home/ubuntu/interview
            echo "PY_TEST=${{ secrets.PY_TEST }}" >> .env

            docker-compose stop ml-server ml-worker || true
            docker-compose rm -f ml-server ml-worker || true

            docker system prune -af

            echo "📦 ML 이미지 pull"
            docker-compose pull ml-server ml-worker

            echo "🚀 ML 서버 재시작"
            docker-compose up -d --build ml-server ml-worker

            echo "✅ ML 서버 배포 완료!"
